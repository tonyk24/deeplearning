# -*- coding: utf-8 -*-
"""Reinforcement Learning Pole Cart Gym.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hOTmbBSiZS1Ea77_7iwwpayvcjLNDDfZ
"""

# Inspired by RL researchers and written by Kalle Prorok for a Deep Learning Course
# Ume√• Sweden 2019
import random
import gym
import matplotlib.pyplot as plt

EPISODES = 10000
EPSILON = 0.2
GAMMA = 0.8
LEARNING_RATE = 0.1
DISCRETE_STEPS = 10     # 10 discretization steps per state variable

def argmax(l):
    """ Return the index of the maximum element of a list
    """
    return max(enumerate(l), key=lambda x:x[1])[0]

def make_state(observation):
    """ Map a 4-dimensional state to a state index
    """
	#Num	Observation
    #  0	Cart Position
    #  1	Cart Velocity
    #  2	Pole Angle
    #  3	Pole Velocity At Tip
    low = [-4.8, -10., -0.41, -10.]
    high = [4.8, 10., 0.41, 10.]
    state = 0

    for i in range(4):
        # State variable, projected to the [0, 1] range
        state_variable = (observation[i] - low[i]) / (high[i] - low[i])

        # Discretize. A variable having a value of 0.53 will lead to the integer 5,
        # for instance.
        state_discrete = int(state_variable * DISCRETE_STEPS)
        state_discrete = max(0, state_discrete)
        state_discrete = min(DISCRETE_STEPS-1, state_discrete)

        state *= DISCRETE_STEPS
        state += state_discrete

    return state

def main():
    average_cumulative_reward = 0.0

    # Create the Gym environment (CartPole)
    env = gym.make('CartPole-v1')

	# Type Discrete(2)
	# Num   Action
	# 0     Push cart to left
	# 1     Push cart to right
    print('Action space is:', env.action_space)
	
	# Type Box(4)
    #Num	Observation                 Min         Max
    #  0	Cart Position             -4.8            4.8
    #  1	Cart Velocity             -Inf            Inf
    #  2	Pole Angle                 -24 deg        24 deg
    #  3	Pole Velocity At Tip      -Inf            Inf
    print('Observation space is:', env.observation_space)

    # Q-table for the discretized states, and two actions
    num_states = DISCRETE_STEPS ** 4
    qtable = [[0., 0.] for state in range(num_states)]
	
    reward_list = []

    # Loop over episodes
    for i in range(EPISODES):
        state = env.reset()
        state = make_state(state)

        terminate = False
        cumulative_reward = 0.0

        # Loop over time-steps
        while not terminate:
            # Compute what the greedy action for the current state is
            qvalues = qtable[state]
			# One or zero depending on which one is largest
            greedy_action = argmax(qvalues)
            #print(str(qtable[state]))
            #print("theone: " + str(greedy_action))

            # Sometimes, the agent takes a random action, to explore the environment
			# random() returns float in range [0.0, 1.0]
            if random.random() < EPSILON:
                action = random.randrange(2)
            else:
                action = greedy_action

            # Perform the action
            next_state, reward, terminate, info = env.step(action)  # info is ignored
            next_state = make_state(next_state)
            
			# Show the simulated environment. A bit difficult to make it work.            
            #env.render()
            #print(' Reward:',reward)
            # Update the Q-Table
            td_error = reward + GAMMA * max(qtable[next_state]) - qtable[state][action]
            qtable[state][action] += LEARNING_RATE * td_error
            # Update statistics
            cumulative_reward += reward
            state = next_state
        
        # Per-episode statistics
        reward_list.append(cumulative_reward)
        if ((i % 100)==0):
          print(i, cumulative_reward, sep=',')
    plt.plot(reward_list)
    plt.ylabel('Reward')
    plt.show()
	
#if __name__ == '__main__':
main()